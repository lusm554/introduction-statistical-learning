# [An Introduction to Statistical Learning](https://www.statlearning.com/)

# Notes from ISLP

## Reducible / Irreducible error definition in estimating f
<b> Reducible error (сокращаемая ошибка) - </b> ошибка, которую можно уменьшить или устранить с помощью выбора более подходящей модели или улучшения метода обучения. <br>

К ней относятся две компоненты:
- Bias (смещение): ошибка, которая возникает из-за того, что модель слишком упрощена или недостаточно гибка для адекватного описания данных. Чем проще модель, тем выше смещение.
- Variance (дисперсия): ошибка, которая возникает, когда модель слишком сложная и "переподстраивается" под шум в данных. Модели с высокой дисперсией показывают высокую чувствительность к изменениям в обучающих данных.

Сокращаемая ошибка зависит от выбранной модели и методов обучения, и ее можно уменьшить с помощью правильного выбора алгоритма, настройки гиперпараметров, использования регуляризации и других методов.

<b> Irreducible error (несокращаемая ошибка) - </b> ошибка, которая является неотъемлемой частью задачи из-за случайных колебаний или шумов в данных. Эта ошибка не зависит от модели или алгоритма обучения, поскольку она вызвана факторами, которые невозможно контролировать, например, случайными ошибками измерений или неизвестными переменными, влияющими на результат.


## Prediction / Inference definition

<b>Prediction (предсказание):</b>

- Цель: Создание модели, которая способна точно предсказывать значение целевой переменной (Y) для новых, невиданных данных.
- Фокус: На точности предсказания. Здесь не обязательно понимать, как конкретные переменные влияют на результат.

<b>Inference (вывод):</b>

- Цель: Понять взаимосвязи между переменными, интерпретировать влияние каждой предикторной переменной (X) на целевую переменную (Y).
- Фокус: На интерпретации.


## Parametric and Non-parametric methods definition

<b> Parametric Methods (Параметрические методы) </b>

- Фиксированное количество параметров: Параметрические методы предполагают, что данные могут быть описаны с помощью конечного набора параметров. Например, в линейной регрессии модель описывается параметрами B0, B1, B2...
- Гипотезы о распределении данных: Часто предполагается, что данные следуют определенному распределению, например, нормальному. Эти предположения помогают упростить расчет и интерпретацию.
- Быстрая работа: За счет малого числа параметров эти методы обычно работают быстрее и требуют меньше данных.

Преимущества:
- Простота и интерпретируемость.
- Работают хорошо, если предположения о данных верны.

Недостатки:
- Если данные не соответствуют предположениям модели, это может привести к сильным отклонениям в результатах.
- Не такие гибкие, как непараметрические методы.

Примеры:
- Линейная регрессия.
- Логистическая регрессия.
- Дисперсионный анализ (ANOVA).
- Гауссовы процессы (с предположением нормального распределения).

<b> Non-Parametric Methods (Непараметрические методы) </b>

- Не фиксируют число параметров: Эти методы не предполагают фиксированную форму или структуру данных. Вместо этого они позволяют данным "говорить самим за себя".
- Гибкость: Non-parametric методы могут приспосабливаться к любой структуре данных, так как не делают сильных предположений о форме распределения.
- Больше данных: Для корректной работы непараметрические методы обычно требуют большего количества данных, поскольку гибкость приводит к риску переобучения.

Преимущества:
- Работают в ситуациях, когда данные не соответствуют предположениям, используемым параметрическими методами.
- Более универсальны, особенно при работе с большими и сложными наборами данных.

Недостатки:
- Более медленные и вычислительно затратные из-за отсутствия предположений о данных.
- Менее интерпретируемы по сравнению с параметрическими методами.

Примеры:
- K-ближайших соседей (k-NN).
- Решающие деревья (Decision Trees).
- Ядерные методы оценки плотности (Kernel Density Estimation).
- Непараметрические статистические тесты (например, критерий Манна-Уитни).



