# [An Introduction to Statistical Learning](https://www.statlearning.com/)

# Notes from ISLP

## Reducible / Irreducible error definition in estimating f
<b> Reducible error (сокращаемая ошибка) - </b> ошибка, которую можно уменьшить или устранить с помощью выбора более подходящей модели или улучшения метода обучения. <br>

К ней относятся две компоненты:
- Bias (смещение): ошибка, которая возникает из-за того, что модель слишком упрощена или недостаточно гибка для адекватного описания данных. Чем проще модель, тем выше смещение.
- Variance (дисперсия): ошибка, которая возникает, когда модель слишком сложная и "переподстраивается" под шум в данных. Модели с высокой дисперсией показывают высокую чувствительность к изменениям в обучающих данных.

Сокращаемая ошибка зависит от выбранной модели и методов обучения, и ее можно уменьшить с помощью правильного выбора алгоритма, настройки гиперпараметров, использования регуляризации и других методов.

<b> Irreducible error (несокращаемая ошибка) - </b> ошибка, которая является неотъемлемой частью задачи из-за случайных колебаний или шумов в данных. Эта ошибка не зависит от модели или алгоритма обучения, поскольку она вызвана факторами, которые невозможно контролировать, например, случайными ошибками измерений или неизвестными переменными, влияющими на результат.


## Prediction / Inference definition

<b>Prediction (предсказание):</b>

- Цель: Создание модели, которая способна точно предсказывать значение целевой переменной (Y) для новых, невиданных данных.
- Фокус: На точности предсказания. Здесь не обязательно понимать, как конкретные переменные влияют на результат.

<b>Inference (вывод):</b>

- Цель: Понять взаимосвязи между переменными, интерпретировать влияние каждой предикторной переменной (X) на целевую переменную (Y).
- Фокус: На интерпретации.


## Parametric and Non-parametric methods definition

<b> Parametric Methods (Параметрические методы) </b>

- Фиксированное количество параметров: Параметрические методы предполагают, что данные могут быть описаны с помощью конечного набора параметров. Например, в линейной регрессии модель описывается параметрами B0, B1, B2...
- Гипотезы о распределении данных: Часто предполагается, что данные следуют определенному распределению, например, нормальному. Эти предположения помогают упростить расчет и интерпретацию.
- Быстрая работа: За счет малого числа параметров эти методы обычно работают быстрее и требуют меньше данных.

Преимущества:
- Простота и интерпретируемость.
- Работают хорошо, если предположения о данных верны.

Недостатки:
- Если данные не соответствуют предположениям модели, это может привести к сильным отклонениям в результатах.
- Не такие гибкие, как непараметрические методы.

Примеры:
- Линейная регрессия.
- Логистическая регрессия.
- Дисперсионный анализ (ANOVA).
- Гауссовы процессы (с предположением нормального распределения).

<b> Non-Parametric Methods (Непараметрические методы) </b>

- Не фиксируют число параметров: Эти методы не предполагают фиксированную форму или структуру данных. Вместо этого они позволяют данным "говорить самим за себя".
- Гибкость: Non-parametric методы могут приспосабливаться к любой структуре данных, так как не делают сильных предположений о форме распределения.
- Больше данных: Для корректной работы непараметрические методы обычно требуют большего количества данных, поскольку гибкость приводит к риску переобучения.

Преимущества:
- Работают в ситуациях, когда данные не соответствуют предположениям, используемым параметрическими методами.
- Более универсальны, особенно при работе с большими и сложными наборами данных.

Недостатки:
- Более медленные и вычислительно затратные из-за отсутствия предположений о данных.
- Менее интерпретируемы по сравнению с параметрическими методами.

Примеры:
- K-ближайших соседей (k-NN).
- Решающие деревья (Decision Trees).
- Ядерные методы оценки плотности (Kernel Density Estimation).
- Непараметрические статистические тесты (например, критерий Манна-Уитни).

## Bias-variance trade-of

Основная идея bias-variance trade-off:

* Смещение (bias): Ошибка из-за упрощённых предположений модели (например, линейная модель для сложных данных). Высокое смещение приводит к недообучению.
* Разброс (variance): Ошибка из-за высокой чувствительности модели к данным (переобучение). Сложные модели с низким смещением часто имеют высокий разброс.
* Цель: Найти баланс между смещением и разбросом, чтобы минимизировать общую ошибку (смещение² + разброс + шум).

Упрощённо:
* Слишком простая модель → Недообучение (высокое смещение, низкий разброс).
* Слишком сложная модель → Переобучение (низкое смещение, высокий разброс).

Решение: Регуляризация, кросс-валидация и выбор оптимальной сложности модели помогают найти этот баланс.

## Bayes error rate

Bayes error rate — это минимальная возможная ошибка классификации, которую можно достичь, если бы мы знали истинное распределение вероятностей Pr(Y=j∣X).

Bayes error rate — это теоретический нижний предел ошибок для любой классификационной модели.
Он показывает, насколько сложно отделить классы друг от друга при данном распределении данных.

Bayes error rate — это теоретический ориентир, который показывает, как близко ваш классификатор (например, логистическая регрессия, решающее дерево или нейросеть) подходит к идеальной модели.
Если ошибка модели значительно выше Bayes error rate, это значит, что:

Модель недостаточно сложная или плохо обучена.
Возможно, признаки X не содержат достаточной информации для разделения классов.

## Residual Standard Error (RSE)

RSE — это оценка стандартного отклонения ошибок (остатков) в модели линейной регрессии. Остатки представляют собой разницу между наблюдаемыми значениями зависимой переменной yi и предсказанными значениями ^yi.

Интерпретация RSE:
* RSE измеряет среднее отклонение наблюдаемых значений от линии регрессии.
* Чем меньше RSE, тем лучше модель предсказывает данные.
* RSE выражается в тех же единицах, что и зависимая переменная y, что делает его интерпретируемым.

## R² (коэффициент детерминации)

R² — это мера того, насколько хорошо модель объясняет изменчивость зависимой переменной. Он показывает долю дисперсии y, которая объясняется предикторами модели. Другими словами, R² отвечает на вопрос: "Какую долю изменчивости y можно объяснить с помощью предикторов модели?"

<b>Как интерпретировать R²?</b>
* R² принимает значения от 0 до 1:
	- R2=0: Модель не объясняет изменчивость y. Это означает, что модель работает не лучше, чем простое предсказание средним значением -y.
	- R2=1: Модель идеально объясняет всю изменчивость y. Все точки данных лежат на линии регрессии.
	- Чем ближе R² к 1, тем лучше модель объясняет данные.
* Пример:
	- Если R2=0.8, это означает, что 80% изменчивости зависимой переменной y объясняется предикторами модели, а оставшиеся 20% — это необъясненная случайная ошибка.


<b> Нюансы R² </b>
1. R² не говорит о причинно-следственной связи:
	* Высокий R² не означает, что предикторы действительно вызывают изменения в y. Это просто мера корреляции.
2. R² может быть misleading (вводить в заблуждение) при добавлении предикторов:
	* Если добавить в модель больше предикторов, R² всегда увеличивается (или остается неизменным), даже если новые предикторы не имеют реальной объясняющей силы. Поэтому в моделях с большим количеством предикторов лучше использовать скорректированный R² (Adjusted R²), который учитывает количество предикторов.
3. R² зависит от контекста:
	* В некоторых областях (например, социальные науки) даже низкий R² (например, 0.2) может считаться хорошим результатом, если данные очень шумные. В других областях (например, физика) ожидаются гораздо более высокие значения R².
